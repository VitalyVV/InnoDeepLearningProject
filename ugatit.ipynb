{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ugatit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-oGnRCVRvic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOA0cQ5mRxpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd gdrive/My Drive/\n",
        "# %ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDDO4i5jYiNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, itertools\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from glob import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD8ewg0yYiNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=6, img_size=256, light=False):\n",
        "        assert(n_blocks >= 0)\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "        self.input_nc = input_nc\n",
        "        self.output_nc = output_nc\n",
        "        self.ngf = ngf\n",
        "        self.n_blocks = n_blocks\n",
        "        self.img_size = img_size\n",
        "        self.light = light\n",
        "\n",
        "        DownBlock = []\n",
        "        DownBlock += [nn.ReflectionPad2d(3),\n",
        "                      nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "                      nn.InstanceNorm2d(ngf),\n",
        "                      nn.ReLU(True)]\n",
        "\n",
        "        # Down-Sampling\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**i\n",
        "            DownBlock += [nn.ReflectionPad2d(1),\n",
        "                          nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=0, bias=False),\n",
        "                          nn.InstanceNorm2d(ngf * mult * 2),\n",
        "                          nn.ReLU(True)]\n",
        "\n",
        "        # Down-Sampling Bottleneck\n",
        "        mult = 2**n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            DownBlock += [ResnetBlock(ngf * mult, use_bias=False)]\n",
        "\n",
        "        # Class Activation Map\n",
        "        self.gap_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
        "        self.gmp_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
        "        self.conv1x1 = nn.Conv2d(ngf * mult * 2, ngf * mult, kernel_size=1, stride=1, bias=True)\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        # Gamma, Beta block\n",
        "        if self.light:\n",
        "            FC = [nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True)]\n",
        "        else:\n",
        "            FC = [nn.Linear(img_size // mult * img_size // mult * ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True),\n",
        "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
        "                  nn.ReLU(True)]\n",
        "        self.gamma = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
        "        self.beta = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
        "\n",
        "        # Up-Sampling Bottleneck\n",
        "        for i in range(n_blocks):\n",
        "            setattr(self, 'UpBlock1_' + str(i+1), ResnetAdaILNBlock(ngf * mult, use_bias=False))\n",
        "\n",
        "        # Up-Sampling\n",
        "        UpBlock2 = []\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2**(n_downsampling - i)\n",
        "            UpBlock2 += [nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                         nn.ReflectionPad2d(1),\n",
        "                         nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=1, padding=0, bias=False),\n",
        "                         ILN(int(ngf * mult / 2)),\n",
        "                         nn.ReLU(True)]\n",
        "\n",
        "        UpBlock2 += [nn.ReflectionPad2d(3),\n",
        "                     nn.Conv2d(ngf, output_nc, kernel_size=7, stride=1, padding=0, bias=False),\n",
        "                     nn.Tanh()]\n",
        "\n",
        "        self.DownBlock = nn.Sequential(*DownBlock)\n",
        "        self.FC = nn.Sequential(*FC)\n",
        "        self.UpBlock2 = nn.Sequential(*UpBlock2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.DownBlock(input)\n",
        "\n",
        "        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n",
        "        gap_weight = list(self.gap_fc.parameters())[0]\n",
        "        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n",
        "        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n",
        "        gmp_weight = list(self.gmp_fc.parameters())[0]\n",
        "        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n",
        "        x = torch.cat([gap, gmp], 1)\n",
        "        x = self.relu(self.conv1x1(x))\n",
        "\n",
        "        heatmap = torch.sum(x, dim=1, keepdim=True)\n",
        "\n",
        "        if self.light:\n",
        "            x_ = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "            x_ = self.FC(x_.view(x_.shape[0], -1))\n",
        "        else:\n",
        "            x_ = self.FC(x.view(x.shape[0], -1))\n",
        "        gamma, beta = self.gamma(x_), self.beta(x_)\n",
        "\n",
        "\n",
        "        for i in range(self.n_blocks):\n",
        "            x = getattr(self, 'UpBlock1_' + str(i+1))(x, gamma, beta)\n",
        "        out = self.UpBlock2(x)\n",
        "\n",
        "        return out, cam_logit, heatmap\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, use_bias):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        conv_block = []\n",
        "        conv_block += [nn.ReflectionPad2d(1),\n",
        "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
        "                       nn.InstanceNorm2d(dim),\n",
        "                       nn.ReLU(True)]\n",
        "\n",
        "        conv_block += [nn.ReflectionPad2d(1),\n",
        "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
        "                       nn.InstanceNorm2d(dim)]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResnetAdaLINBlock(nn.Module):\n",
        "    def __init__(self, dim, use_bias):\n",
        "        super(ResnetAdaLINBlock, self).__init__()\n",
        "        self.pad1 = nn.ReflectionPad2d(1)\n",
        "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n",
        "        self.norm1 = adaILN(dim)\n",
        "        self.relu1 = nn.ReLU(True)\n",
        "\n",
        "        self.pad2 = nn.ReflectionPad2d(1)\n",
        "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n",
        "        self.norm2 = adaILN(dim)\n",
        "\n",
        "    def forward(self, x, gamma, beta):\n",
        "        out = self.pad1(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm1(out, gamma, beta)\n",
        "        out = self.relu1(out)\n",
        "        out = self.pad2(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm2(out, gamma, beta)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "\n",
        "class adaLIN(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super(adaLIN, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        self.rho.data.fill_(0.9)\n",
        "\n",
        "    def forward(self, input, gamma, beta):\n",
        "        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n",
        "        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n",
        "        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n",
        "        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n",
        "        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n",
        "        out = out * gamma.unsqueeze(2).unsqueeze(3) + beta.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ILN(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super(ILN, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
        "        self.rho.data.fill_(0.0)\n",
        "        self.gamma.data.fill_(1.0)\n",
        "        self.beta.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n",
        "        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n",
        "        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n",
        "        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n",
        "        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n",
        "        out = out * self.gamma.expand(input.shape[0], -1, -1, -1) + self.beta.expand(input.shape[0], -1, -1, -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=5):\n",
        "        super(Discriminator, self).__init__()\n",
        "        model = [nn.ReflectionPad2d(1),\n",
        "                 nn.utils.spectral_norm(\n",
        "                 nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=0, bias=True)),\n",
        "                 nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        for i in range(1, n_layers - 2):\n",
        "            mult = 2 ** (i - 1)\n",
        "            model += [nn.ReflectionPad2d(1),\n",
        "                      nn.utils.spectral_norm(\n",
        "                      nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=2, padding=0, bias=True)),\n",
        "                      nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        mult = 2 ** (n_layers - 2 - 1)\n",
        "        model += [nn.ReflectionPad2d(1),\n",
        "                  nn.utils.spectral_norm(\n",
        "                  nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=1, padding=0, bias=True)),\n",
        "                  nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        # Class Activation Map\n",
        "        mult = 2 ** (n_layers - 2)\n",
        "        self.gap_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n",
        "        self.gmp_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n",
        "        self.conv1x1 = nn.Conv2d(ndf * mult * 2, ndf * mult, kernel_size=1, stride=1, bias=True)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        self.pad = nn.ReflectionPad2d(1)\n",
        "        self.conv = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(ndf * mult, 1, kernel_size=4, stride=1, padding=0, bias=False))\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.model(input)\n",
        "\n",
        "        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
        "        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n",
        "        gap_weight = list(self.gap_fc.parameters())[0]\n",
        "        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n",
        "        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n",
        "        gmp_weight = list(self.gmp_fc.parameters())[0]\n",
        "        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n",
        "        x = torch.cat([gap, gmp], 1)\n",
        "        x = self.leaky_relu(self.conv1x1(x))\n",
        "\n",
        "        heatmap = torch.sum(x, dim=1, keepdim=True)\n",
        "\n",
        "        x = self.pad(x)\n",
        "        out = self.conv(x)\n",
        "\n",
        "        return out, cam_logit, heatmap\n",
        "\n",
        "\n",
        "class RhoClipper(object):\n",
        "\n",
        "    def __init__(self, min, max):\n",
        "        self.clip_min = min\n",
        "        self.clip_max = max\n",
        "        assert min < max\n",
        "\n",
        "    def __call__(self, module):\n",
        "\n",
        "        if hasattr(module, 'rho'):\n",
        "            w = module.rho.data\n",
        "            w = w.clamp(self.clip_min, self.clip_max)\n",
        "            module.rho.data = w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8U-2JBiyR4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cam(x, size = 256):\n",
        "    x = x - np.min(x)\n",
        "    cam_img = x / np.max(x)\n",
        "    cam_img = np.uint8(255 * cam_img)\n",
        "    cam_img = cv2.resize(cam_img, (size, size))\n",
        "    cam_img = cv2.applyColorMap(cam_img, cv2.COLORMAP_JET)\n",
        "    return cam_img / 255.0\n",
        "  \n",
        "def denorm(x):\n",
        "  return x * 0.5 + 0.5\n",
        "\n",
        "def tensor2numpy(x):\n",
        "    return x.detach().cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "def RGB2BGR(x):\n",
        "    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtxhFdpsnVPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "\n",
        "def has_file_allowed_extension(filename, extensions):\n",
        "    \"\"\"Checks if a file is an allowed extension.\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "    Returns:\n",
        "        bool: True if the filename ends with a known image extension\n",
        "    \"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "    return any(filename_lower.endswith(ext) for ext in extensions)\n",
        "\n",
        "\n",
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "def make_dataset(dir, extensions):\n",
        "    images = []\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in sorted(fnames):\n",
        "            if has_file_allowed_extension(fname, extensions):\n",
        "                path = os.path.join(root, fname)\n",
        "                item = (path, 0)\n",
        "                images.append(item)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "class DatasetFolder(data.Dataset):\n",
        "    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n",
        "        # classes, class_to_idx = find_classes(root)\n",
        "        samples = make_dataset(root, extensions)\n",
        "        if len(samples) == 0:\n",
        "            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n",
        "                               \"Supported extensions are: \" + \",\".join(extensions)))\n",
        "\n",
        "        self.root = root\n",
        "        self.loader = loader\n",
        "        self.extensions = extensions\n",
        "        self.samples = samples\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (sample, target) where target is class_index of the target class.\n",
        "        \"\"\"\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            sample = self.transform(sample)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "      \n",
        "    def __repr__(self):\n",
        "        fmt_str  = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp      = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp      = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str\n",
        "\n",
        "\n",
        "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return pil_loader(path)\n",
        "\n",
        "\n",
        "class ImageFolder(DatasetFolder):\n",
        "    def __init__(self, root, transform=None, target_transform=None,\n",
        "                 loader=default_loader):\n",
        "        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n",
        "                                          transform=transform,\n",
        "                                          target_transform=target_transform)\n",
        "        self.imgs = self.samples\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0ZtxVnYiNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UGATIT(object) :\n",
        "    def __init__(self, device='cuda', dataset='test', resume=True):\n",
        "        self.img_size = 256\n",
        "        self.ch = 64\n",
        "        self.device = device\n",
        "        self.lr = 0.0001\n",
        "        self.weight_decay = 0.0001\n",
        "        self.n_res = 4\n",
        "        self.n_dis = 6\n",
        "        self.iteration = 1000000\n",
        "        self.batch_size = 1\n",
        "        self.print_freq = 10000\n",
        "        self.save_freq = 1000\n",
        "        self.adv_weight = 1\n",
        "        self.cycle_weight = 10\n",
        "        self.identity_weight = 10\n",
        "        self.cam_weight = 1000\n",
        "        self.result_dir = 'results'\n",
        "        self.light = True\n",
        "        self.dataset = dataset\n",
        "        self.resume = False\n",
        "        self.decay_flag = False\n",
        "        self.resume = resume\n",
        "        self.model_name = 'UGATIT'\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        \n",
        "        \n",
        "    \n",
        "    def build_model(self):\n",
        "        \"\"\" DataLoader \"\"\"\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.Resize((self.img_size + 30, self.img_size+30)),\n",
        "            transforms.RandomCrop(self.img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize((self.img_size, self.img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        self.trainA = ImageFolder(os.path.join('datasets', self.dataset, 'TrainA'), train_transform)\n",
        "        self.trainB = ImageFolder(os.path.join('datasets', self.dataset, 'TrainB'), train_transform)\n",
        "        self.testA = ImageFolder(os.path.join('datasets', self.dataset, 'TestA'), test_transform)\n",
        "        self.testB = ImageFolder(os.path.join('datasets', self.dataset, 'TestB'), test_transform)\n",
        "        self.trainA_loader = DataLoader(self.trainA, batch_size=self.batch_size, shuffle=True)\n",
        "        self.trainB_loader = DataLoader(self.trainB, batch_size=self.batch_size, shuffle=True)\n",
        "        self.testA_loader = DataLoader(self.testA, batch_size=1, shuffle=False)\n",
        "        self.testB_loader = DataLoader(self.testB, batch_size=1, shuffle=False)\n",
        "        \n",
        "        \"\"\" Define Generator, Discriminator \"\"\"\n",
        "        self.genA2B = ResnetGenerator(input_nc=3, output_nc=3, ngf=self.ch, n_blocks=self.n_res, img_size=self.img_size, light=self.light).to(self.device)\n",
        "        self.genB2A = ResnetGenerator(input_nc=3, output_nc=3, ngf=self.ch, n_blocks=self.n_res, img_size=self.img_size, light=self.light).to(self.device)\n",
        "        self.disGA = Discriminator(input_nc=3, ndf=self.ch, n_layers=7).to(self.device)\n",
        "        self.disGB = Discriminator(input_nc=3, ndf=self.ch, n_layers=7).to(self.device)\n",
        "        self.disLA = Discriminator(input_nc=3, ndf=self.ch, n_layers=5).to(self.device)\n",
        "        self.disLB = Discriminator(input_nc=3, ndf=self.ch, n_layers=5).to(self.device)\n",
        "\n",
        "        \"\"\" Define Loss \"\"\"\n",
        "        self.L1_loss = nn.L1Loss().to(self.device)\n",
        "        self.MSE_loss = nn.MSELoss().to(self.device)\n",
        "        self.BCE_loss = nn.BCEWithLogitsLoss().to(self.device)\n",
        "\n",
        "        \"\"\" Trainer \"\"\"\n",
        "        self.G_optim = torch.optim.Adam(itertools.chain(self.genA2B.parameters(), self.genB2A.parameters()), lr=self.lr, betas=(0.5, 0.999), weight_decay=self.weight_decay)\n",
        "        self.D_optim = torch.optim.Adam(itertools.chain(self.disGA.parameters(), self.disGB.parameters(), self.disLA.parameters(), self.disLB.parameters()), lr=self.lr, betas=(0.5, 0.999), weight_decay=self.weight_decay)\n",
        "\n",
        "        \"\"\" Define Rho clipper to constraint the value of rho in AdaILN and ILN\"\"\"\n",
        "        self.Rho_clipper = RhoClipper(0, 1)\n",
        "    \n",
        "    def save(self, dir, step):\n",
        "        params = {}\n",
        "        params['genA2B'] = self.genA2B.state_dict()\n",
        "        params['genB2A'] = self.genB2A.state_dict()\n",
        "        params['disGA'] = self.disGA.state_dict()\n",
        "        params['disGB'] = self.disGB.state_dict()\n",
        "        params['disLA'] = self.disLA.state_dict()\n",
        "        params['disLB'] = self.disLB.state_dict()\n",
        "        torch.save(params, os.path.join(dir, self.dataset + '_params_%07d.pt' % step))\n",
        "\n",
        "\n",
        "    def load(self, dir, step):\n",
        "        params = torch.load(os.path.join(dir, self.dataset + '_params_%07d.pt' % step))\n",
        "        self.genA2B.load_state_dict(params['genA2B'])\n",
        "        self.genB2A.load_state_dict(params['genB2A'])\n",
        "        self.disGA.load_state_dict(params['disGA'])\n",
        "        self.disGB.load_state_dict(params['disGB'])\n",
        "        self.disLA.load_state_dict(params['disLA'])\n",
        "        self.disLB.load_state_dict(params['disLB'])\n",
        "\n",
        "        \n",
        "    def train(self):\n",
        "        self.genA2B.train(), self.genB2A.train(), self.disGA.train(), self.disGB.train(), self.disLA.train(), self.disLB.train()\n",
        "\n",
        "        start_iter = 1\n",
        "        if self.resume:\n",
        "            model_list = glob(os.path.join(self.result_dir, self.dataset, 'model', '*.pt'))\n",
        "            if not len(model_list) == 0:\n",
        "                model_list.sort()\n",
        "                start_iter = int(model_list[-1].split('_')[-1].split('.')[0])\n",
        "                self.load(os.path.join(self.result_dir, self.dataset, 'model'), start_iter)\n",
        "                print(\" [*] Load SUCCESS\")\n",
        "                if self.decay_flag and start_iter > (self.iteration // 2):\n",
        "                    self.G_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2)) * (start_iter - self.iteration // 2)\n",
        "                    self.D_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2)) * (start_iter - self.iteration // 2)\n",
        "\n",
        "        # training loop\n",
        "        print('start training!')\n",
        "        start_time = time.time()\n",
        "        # For iterations\n",
        "        for step in range(start_iter, self.iteration + 1):\n",
        "          # If decay specified and half of the training passed\n",
        "            if self.decay_flag and step > (self.iteration // 2):\n",
        "              # Slow down the training process\n",
        "                self.G_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2))\n",
        "                self.D_optim.param_groups[0]['lr'] -= (self.lr / (self.iteration // 2))\n",
        "            \n",
        "            # Safe load\n",
        "            try:\n",
        "                real_A, _ = trainA_iter.next()\n",
        "            except:\n",
        "                trainA_iter = iter(self.trainA_loader)\n",
        "                real_A, _ = trainA_iter.next()\n",
        "\n",
        "            try:\n",
        "                real_B, _ = trainB_iter.next()\n",
        "            except:\n",
        "                trainB_iter = iter(self.trainB_loader)\n",
        "                real_B, _ = trainB_iter.next()\n",
        "            \n",
        "            # Load training images\n",
        "            real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n",
        "\n",
        "            # Update Discriminator\n",
        "            self.D_optim.zero_grad()\n",
        "            \n",
        "            # Generate fakes\n",
        "            fake_A2B, _, _ = self.genA2B(real_A)\n",
        "            fake_B2A, _, _ = self.genB2A(real_B)\n",
        "            \n",
        "            # Get measures for real \n",
        "            real_GA_logit, real_GA_cam_logit, _ = self.disGA(real_A)\n",
        "            real_LA_logit, real_LA_cam_logit, _ = self.disLA(real_A)\n",
        "            real_GB_logit, real_GB_cam_logit, _ = self.disGB(real_B)\n",
        "            real_LB_logit, real_LB_cam_logit, _ = self.disLB(real_B)\n",
        "             \n",
        "            # Get measures for fakes\n",
        "            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n",
        "            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n",
        "            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n",
        "            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n",
        "            \n",
        "            \n",
        "            # TODO: Leave one CAM loss\n",
        "            D_ad_loss_GA     = self.MSE_loss(real_GA_logit, torch.ones_like(real_GA_logit).to(self.device)) + self.MSE_loss(fake_GA_logit, torch.zeros_like(fake_GA_logit).to(self.device))\n",
        "            D_ad_cam_loss_GA = self.MSE_loss(real_GA_cam_logit, torch.ones_like(real_GA_cam_logit).to(self.device)) + self.MSE_loss(fake_GA_cam_logit, torch.zeros_like(fake_GA_cam_logit).to(self.device))\n",
        "            D_ad_loss_LA     = self.MSE_loss(real_LA_logit, torch.ones_like(real_LA_logit).to(self.device)) + self.MSE_loss(fake_LA_logit, torch.zeros_like(fake_LA_logit).to(self.device))\n",
        "            D_ad_cam_loss_LA = self.MSE_loss(real_LA_cam_logit, torch.ones_like(real_LA_cam_logit).to(self.device)) + self.MSE_loss(fake_LA_cam_logit, torch.zeros_like(fake_LA_cam_logit).to(self.device))\n",
        "            D_ad_loss_GB     = self.MSE_loss(real_GB_logit, torch.ones_like(real_GB_logit).to(self.device)) + self.MSE_loss(fake_GB_logit, torch.zeros_like(fake_GB_logit).to(self.device))\n",
        "            D_ad_cam_loss_GB = self.MSE_loss(real_GB_cam_logit, torch.ones_like(real_GB_cam_logit).to(self.device)) + self.MSE_loss(fake_GB_cam_logit, torch.zeros_like(fake_GB_cam_logit).to(self.device))\n",
        "            D_ad_loss_LB     = self.MSE_loss(real_LB_logit, torch.ones_like(real_LB_logit).to(self.device)) + self.MSE_loss(fake_LB_logit, torch.zeros_like(fake_LB_logit).to(self.device))\n",
        "            D_ad_cam_loss_LB = self.MSE_loss(real_LB_cam_logit, torch.ones_like(real_LB_cam_logit).to(self.device)) + self.MSE_loss(fake_LB_cam_logit, torch.zeros_like(fake_LB_cam_logit).to(self.device))\n",
        "            \n",
        "            # Get loss for images\n",
        "            D_loss_A = self.adv_weight * (D_ad_loss_GA + D_ad_cam_loss_GA + D_ad_loss_LA + D_ad_cam_loss_LA)\n",
        "            D_loss_B = self.adv_weight * (D_ad_loss_GB + D_ad_cam_loss_GB + D_ad_loss_LB + D_ad_cam_loss_LB)\n",
        "            \n",
        "            # Calculate loss for Discriminator\n",
        "            Discriminator_loss = D_loss_A + D_loss_B\n",
        "            Discriminator_loss.backward()\n",
        "            self.D_optim.step()\n",
        "\n",
        "            # Update Generator\n",
        "            self.G_optim.zero_grad()\n",
        "            \n",
        "            # Generate fakes\n",
        "            fake_A2B, fake_A2B_cam_logit, _ = self.genA2B(real_A)\n",
        "            fake_B2A, fake_B2A_cam_logit, _ = self.genB2A(real_B)\n",
        "            \n",
        "            # Generate backward fakes\n",
        "            fake_A2B2A, _, _ = self.genB2A(fake_A2B)\n",
        "            fake_B2A2B, _, _ = self.genA2B(fake_B2A)\n",
        "            \n",
        "            # Generate fake to the same image\n",
        "            fake_A2A, fake_A2A_cam_logit, _ = self.genB2A(real_A)\n",
        "            fake_B2B, fake_B2B_cam_logit, _ = self.genA2B(real_B)\n",
        "            \n",
        "            # Calculate losses\n",
        "            fake_GA_logit, fake_GA_cam_logit, _ = self.disGA(fake_B2A)\n",
        "            fake_LA_logit, fake_LA_cam_logit, _ = self.disLA(fake_B2A)\n",
        "            fake_GB_logit, fake_GB_cam_logit, _ = self.disGB(fake_A2B)\n",
        "            fake_LB_logit, fake_LB_cam_logit, _ = self.disLB(fake_A2B)\n",
        "            \n",
        "            # TODO: Leave one loss?\n",
        "            G_ad_loss_GA     = self.MSE_loss(fake_GA_logit, torch.ones_like(fake_GA_logit).to(self.device))\n",
        "            G_ad_cam_loss_GA = self.MSE_loss(fake_GA_cam_logit, torch.ones_like(fake_GA_cam_logit).to(self.device))\n",
        "            G_ad_loss_LA     = self.MSE_loss(fake_LA_logit, torch.ones_like(fake_LA_logit).to(self.device))\n",
        "            G_ad_cam_loss_LA = self.MSE_loss(fake_LA_cam_logit, torch.ones_like(fake_LA_cam_logit).to(self.device))\n",
        "            G_ad_loss_GB     = self.MSE_loss(fake_GB_logit, torch.ones_like(fake_GB_logit).to(self.device))\n",
        "            G_ad_cam_loss_GB = self.MSE_loss(fake_GB_cam_logit, torch.ones_like(fake_GB_cam_logit).to(self.device))\n",
        "            G_ad_loss_LB     = self.MSE_loss(fake_LB_logit, torch.ones_like(fake_LB_logit).to(self.device))\n",
        "            G_ad_cam_loss_LB = self.MSE_loss(fake_LB_cam_logit, torch.ones_like(fake_LB_cam_logit).to(self.device))\n",
        "        \n",
        "            # Calculate recon loss\n",
        "            G_recon_loss_A = self.L1_loss(fake_A2B2A, real_A)\n",
        "            G_recon_loss_B = self.L1_loss(fake_B2A2B, real_B)\n",
        "            \n",
        "            # Calculate indetity loss\n",
        "            G_identity_loss_A = self.L1_loss(fake_A2A, real_A)\n",
        "            G_identity_loss_B = self.L1_loss(fake_B2B, real_B)\n",
        "            \n",
        "            # Cam loss\n",
        "            G_cam_loss_A = self.BCE_loss(fake_B2A_cam_logit, torch.ones_like(fake_B2A_cam_logit).to(self.device)) + self.BCE_loss(fake_A2A_cam_logit, torch.zeros_like(fake_A2A_cam_logit).to(self.device))\n",
        "            G_cam_loss_B = self.BCE_loss(fake_A2B_cam_logit, torch.ones_like(fake_A2B_cam_logit).to(self.device)) + self.BCE_loss(fake_B2B_cam_logit, torch.zeros_like(fake_B2B_cam_logit).to(self.device))\n",
        "            \n",
        "            # Loss of A, B total\n",
        "            G_loss_A =  self.adv_weight * (G_ad_loss_GA + G_ad_cam_loss_GA + G_ad_loss_LA + G_ad_cam_loss_LA) + self.cycle_weight * G_recon_loss_A + self.identity_weight * G_identity_loss_A + self.cam_weight * G_cam_loss_A\n",
        "            G_loss_B = self.adv_weight * (G_ad_loss_GB + G_ad_cam_loss_GB + G_ad_loss_LB + G_ad_cam_loss_LB) + self.cycle_weight * G_recon_loss_B + self.identity_weight * G_identity_loss_B + self.cam_weight * G_cam_loss_B\n",
        "            \n",
        "            # Finally put the loss together\n",
        "            Generator_loss = G_loss_A + G_loss_B\n",
        "            Generator_loss.backward()\n",
        "            self.G_optim.step()\n",
        "            \n",
        "            # clip parameter of AdaILN and ILN, applied after optimizer step\n",
        "            self.genA2B.apply(self.Rho_clipper)\n",
        "            self.genB2A.apply(self.Rho_clipper)\n",
        "\n",
        "            # Print progress\n",
        "            print(\"[%5d/%5d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" % (step, self.iteration, time.time() - start_time, Discriminator_loss, Generator_loss))\n",
        "            if step % self.print_freq == 0:\n",
        "                train_sample_num = 5\n",
        "                test_sample_num = 5\n",
        "                A2B = np.zeros((self.img_size * 7, 0, 3))\n",
        "                B2A = np.zeros((self.img_size * 7, 0, 3))\n",
        "\n",
        "                self.genA2B.eval(), self.genB2A.eval(), self.disGA.eval(), self.disGB.eval(), self.disLA.eval(), self.disLB.eval()\n",
        "                for _ in range(train_sample_num):\n",
        "                    try:\n",
        "                        real_A, _ = trainA_iter.next()\n",
        "                    except:\n",
        "                        trainA_iter = iter(self.trainA_loader)\n",
        "                        real_A, _ = trainA_iter.next()\n",
        "\n",
        "                    try:\n",
        "                        real_B, _ = trainB_iter.next()\n",
        "                    except:\n",
        "                        trainB_iter = iter(self.trainB_loader)\n",
        "                        real_B, _ = trainB_iter.next()\n",
        "                    real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n",
        "\n",
        "                    fake_A2B, _, fake_A2B_heatmap = self.genA2B(real_A)\n",
        "                    fake_B2A, _, fake_B2A_heatmap = self.genB2A(real_B)\n",
        "\n",
        "                    fake_A2B2A, _, fake_A2B2A_heatmap = self.genB2A(fake_A2B)\n",
        "                    fake_B2A2B, _, fake_B2A2B_heatmap = self.genA2B(fake_B2A)\n",
        "\n",
        "                    fake_A2A, _, fake_A2A_heatmap = self.genB2A(real_A)\n",
        "                    fake_B2B, _, fake_B2B_heatmap = self.genA2B(real_B)\n",
        "\n",
        "                    A2B = np.concatenate((A2B, np.concatenate((RGB2BGR(tensor2numpy(denorm(real_A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2B2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B2A[0])))), 0)), 1)\n",
        "\n",
        "                    B2A = np.concatenate((B2A, np.concatenate((RGB2BGR(tensor2numpy(denorm(real_B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2A2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2A2B[0])))), 0)), 1)\n",
        "\n",
        "                for _ in range(test_sample_num):\n",
        "                    try:\n",
        "                        real_A, _ = testA_iter.next()\n",
        "                    except:\n",
        "                        testA_iter = iter(self.testA_loader)\n",
        "                        real_A, _ = testA_iter.next()\n",
        "\n",
        "                    try:\n",
        "                        real_B, _ = testB_iter.next()\n",
        "                    except:\n",
        "                        testB_iter = iter(self.testB_loader)\n",
        "                        real_B, _ = testB_iter.next()\n",
        "                    real_A, real_B = real_A.to(self.device), real_B.to(self.device)\n",
        "\n",
        "                    fake_A2B, _, fake_A2B_heatmap = self.genA2B(real_A)\n",
        "                    fake_B2A, _, fake_B2A_heatmap = self.genB2A(real_B)\n",
        "\n",
        "                    fake_A2B2A, _, fake_A2B2A_heatmap = self.genB2A(fake_A2B)\n",
        "                    fake_B2A2B, _, fake_B2A2B_heatmap = self.genA2B(fake_B2A)\n",
        "\n",
        "                    fake_A2A, _, fake_A2A_heatmap = self.genB2A(real_A)\n",
        "                    fake_B2B, _, fake_B2B_heatmap = self.genA2B(real_B)\n",
        "\n",
        "                    A2B = np.concatenate((A2B, np.concatenate((RGB2BGR(tensor2numpy(denorm(real_A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_A2B2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_A2B2A[0])))), 0)), 1)\n",
        "\n",
        "                    B2A = np.concatenate((B2A, np.concatenate((RGB2BGR(tensor2numpy(denorm(real_B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2B[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2A_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2A[0]))),\n",
        "                                                               cam(tensor2numpy(fake_B2A2B_heatmap[0]), self.img_size),\n",
        "                                                               RGB2BGR(tensor2numpy(denorm(fake_B2A2B[0])))), 0)), 1)\n",
        "\n",
        "                cv2.imwrite(os.path.join(self.result_dir, self.dataset, 'img', 'A2B_%07d.png' % step), A2B * 255.0)\n",
        "                cv2.imwrite(os.path.join(self.result_dir, self.dataset, 'img', 'B2A_%07d.png' % step), B2A * 255.0)\n",
        "                self.genA2B.train(), self.genB2A.train(), self.disGA.train(), self.disGB.train(), self.disLA.train(), self.disLB.train()\n",
        "\n",
        "            if step % self.save_freq == 0:\n",
        "                self.save(os.path.join(self.result_dir, self.dataset, 'model'), step)\n",
        "\n",
        "            if step % 1000 == 0:\n",
        "                params = {}\n",
        "                params['genA2B'] = self.genA2B.state_dict()\n",
        "                params['genB2A'] = self.genB2A.state_dict()\n",
        "                params['disGA'] = self.disGA.state_dict()\n",
        "                params['disGB'] = self.disGB.state_dict()\n",
        "                params['disLA'] = self.disLA.state_dict()\n",
        "                params['disLB'] = self.disLB.state_dict()\n",
        "                torch.save(params, os.path.join(self.result_dir, self.dataset + '_params_latest.pt'))\n",
        "                \n",
        "                \n",
        "                \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3nDygplYiNy",
        "colab_type": "code",
        "outputId": "52ffefa6-4871-44c6-b6ee-b723a7d1e668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = UGATIT(dataset='animegirls')\n",
        "model.build_model()\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [*] Load SUCCESS\n",
            "start training!\n",
            "[10000/1000000] time: 14.8385 d_loss: 4.17041779, g_loss: 1574.50756836\n",
            "[10001/1000000] time: 37.9925 d_loss: 3.79086471, g_loss: 3477.55664062\n",
            "[10002/1000000] time: 41.0708 d_loss: 2.70584059, g_loss: 1535.80786133\n",
            "[10003/1000000] time: 44.2527 d_loss: 2.44980860, g_loss: 1403.54833984\n",
            "[10004/1000000] time: 47.4813 d_loss: 2.96948910, g_loss: 3671.75659180\n",
            "[10005/1000000] time: 50.5572 d_loss: 3.46248150, g_loss: 3596.82812500\n",
            "[10006/1000000] time: 53.6284 d_loss: 4.11420536, g_loss: 2660.28369141\n",
            "[10007/1000000] time: 56.7658 d_loss: 3.32523918, g_loss: 2082.24340820\n",
            "[10008/1000000] time: 59.7553 d_loss: 3.38649654, g_loss: 1449.72216797\n",
            "[10009/1000000] time: 62.9868 d_loss: 3.22648787, g_loss: 1381.05981445\n",
            "[10010/1000000] time: 66.3209 d_loss: 3.97951865, g_loss: 2611.07080078\n",
            "[10011/1000000] time: 69.7082 d_loss: 4.75126171, g_loss: 2656.09619141\n",
            "[10012/1000000] time: 72.7262 d_loss: 2.53319025, g_loss: 1617.80200195\n",
            "[10013/1000000] time: 75.8043 d_loss: 3.72658277, g_loss: 2523.65771484\n",
            "[10014/1000000] time: 92.1179 d_loss: 3.32058764, g_loss: 1345.93273926\n",
            "[10015/1000000] time: 95.3103 d_loss: 3.03416204, g_loss: 1377.92285156\n",
            "[10016/1000000] time: 98.2212 d_loss: 4.87872934, g_loss: 2052.17871094\n",
            "[10017/1000000] time: 101.2594 d_loss: 3.12353873, g_loss: 1879.88708496\n",
            "[10018/1000000] time: 104.2205 d_loss: 3.94782734, g_loss: 3769.21704102\n",
            "[10019/1000000] time: 107.3698 d_loss: 3.68290615, g_loss: 1674.37744141\n",
            "[10020/1000000] time: 110.3727 d_loss: 4.10200262, g_loss: 1660.30920410\n",
            "[10021/1000000] time: 113.6289 d_loss: 2.48356581, g_loss: 1622.30981445\n",
            "[10022/1000000] time: 116.6953 d_loss: 2.86869383, g_loss: 1333.74096680\n",
            "[10023/1000000] time: 119.7994 d_loss: 4.51901674, g_loss: 1828.65039062\n",
            "[10024/1000000] time: 123.0890 d_loss: 3.65610266, g_loss: 2019.35668945\n",
            "[10025/1000000] time: 126.3038 d_loss: 3.68866158, g_loss: 1374.63232422\n",
            "[10026/1000000] time: 129.5841 d_loss: 3.56166649, g_loss: 1672.60656738\n",
            "[10027/1000000] time: 132.9051 d_loss: 3.94195557, g_loss: 2692.70556641\n",
            "[10028/1000000] time: 136.0155 d_loss: 3.46183681, g_loss: 1499.29736328\n",
            "[10029/1000000] time: 139.1634 d_loss: 3.15055990, g_loss: 3765.84472656\n",
            "[10030/1000000] time: 142.5455 d_loss: 3.05196619, g_loss: 1511.80834961\n",
            "[10031/1000000] time: 145.6514 d_loss: 4.04490566, g_loss: 1761.37634277\n",
            "[10032/1000000] time: 148.6887 d_loss: 4.69426489, g_loss: 1940.07006836\n",
            "[10033/1000000] time: 151.6609 d_loss: 3.26179194, g_loss: 1310.97485352\n",
            "[10034/1000000] time: 154.8694 d_loss: 4.19657612, g_loss: 1796.47558594\n",
            "[10035/1000000] time: 157.9527 d_loss: 3.85574985, g_loss: 1900.21533203\n",
            "[10036/1000000] time: 160.9827 d_loss: 3.30978394, g_loss: 2823.57958984\n",
            "[10037/1000000] time: 164.2953 d_loss: 3.73312235, g_loss: 2083.79394531\n",
            "[10038/1000000] time: 167.8938 d_loss: 3.61992264, g_loss: 1648.32373047\n",
            "[10039/1000000] time: 171.0957 d_loss: 2.81161499, g_loss: 1673.56616211\n",
            "[10040/1000000] time: 174.2057 d_loss: 3.96429157, g_loss: 1843.90869141\n",
            "[10041/1000000] time: 177.8409 d_loss: 3.07208037, g_loss: 1714.22229004\n",
            "[10042/1000000] time: 181.0221 d_loss: 3.80535555, g_loss: 1495.98266602\n",
            "[10043/1000000] time: 184.3241 d_loss: 3.17779565, g_loss: 1469.92993164\n",
            "[10044/1000000] time: 187.5220 d_loss: 3.44211388, g_loss: 1525.99426270\n",
            "[10045/1000000] time: 190.5421 d_loss: 5.19370842, g_loss: 2443.28002930\n",
            "[10046/1000000] time: 193.7663 d_loss: 3.59809017, g_loss: 1742.53564453\n",
            "[10047/1000000] time: 196.8831 d_loss: 3.02442598, g_loss: 1685.81494141\n",
            "[10048/1000000] time: 199.9855 d_loss: 4.36242962, g_loss: 3462.79223633\n",
            "[10049/1000000] time: 203.1952 d_loss: 3.34084749, g_loss: 2089.24438477\n",
            "[10050/1000000] time: 206.4776 d_loss: 2.94282126, g_loss: 1775.79003906\n",
            "[10051/1000000] time: 209.6609 d_loss: 3.87699938, g_loss: 3181.74609375\n",
            "[10052/1000000] time: 212.9139 d_loss: 3.17195845, g_loss: 1983.62951660\n",
            "[10053/1000000] time: 216.1027 d_loss: 2.66259909, g_loss: 3052.91845703\n",
            "[10054/1000000] time: 219.1516 d_loss: 4.12651587, g_loss: 2355.88574219\n",
            "[10055/1000000] time: 222.5470 d_loss: 3.95612025, g_loss: 1605.54809570\n",
            "[10056/1000000] time: 225.6436 d_loss: 3.95021772, g_loss: 1891.78076172\n",
            "[10057/1000000] time: 228.9310 d_loss: 4.31675196, g_loss: 2947.23437500\n",
            "[10058/1000000] time: 232.2361 d_loss: 4.04381609, g_loss: 2019.52612305\n",
            "[10059/1000000] time: 235.5687 d_loss: 3.03018808, g_loss: 1485.29907227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_X4SsqMWour",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
